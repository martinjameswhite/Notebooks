{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating random walks ##\n",
    "\n",
    "When we discussed the Press-Schechter approach to the halo mass function we found that we needed to compute the statistics of a random walk with an absorbing barrier.  This was relatively easy to do by treating the problem as a diffusion equation solved with the method of images.  This is also quite an easy problem to simulate, and that allows us to handle more complex situations (such as curved barriers) and other problems.\n",
    "\n",
    "Random walks of this sort show up in lots of places, and one of the interesting things about them is the generation of power-law tails in the distribution function.  Other places this sort of behavior comes up is in the distribution of the sizes of glacial lakes, distributions of atom \"traps\" in materials or various dynamics problems.\n",
    "\n",
    "A classic \"example\" of a random walk near a barrier is drunks leaving a pub near the ocean, or a canal or river."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drunk near a canal ##\n",
    "\n",
    "Consider an ensemble of drunks leaving a pub ($x=0$), each moving in a single dimension.  We assume that at every step the drunk moves left or right by 1 unit, with the direction being chosen at random.  If there are no constraints, then after $N$ steps the distribution of final locations will be Gaussian (in the limit of large $N$) with a dispersion of $\\sqrt{N}$.  Note that due to the central limit theorem, if the left-right steps are really random then in the large $N$ limit it won't matter if we choose unit steps or steps chosen from a Gaussian or any other distribution with finite mean and variance (recall the derivation of the central limit theorem using the characteristic function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of left-right choices, then sum them to\n",
    "# get the final position.\n",
    "def final_position(Nwalker=1000,Nstep=100):\n",
    "    leftright = np.random.choice([-1.0,1.0],size=(Nwalker,Nstep))\n",
    "    finalpos  = np.sum(leftright,axis=1)\n",
    "    return(finalpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a histogram of the results, and compare this\n",
    "# to a Gaussian of \\sqrt{N}.\n",
    "#\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "nbin   = 25\n",
    "#\n",
    "Nw = 1000\n",
    "Ns = 500\n",
    "fp = final_position(Nw,Ns)\n",
    "ax.hist(fp,bins=nbin,density=True,alpha=0.5)\n",
    "#\n",
    "xx    = np.linspace(-150,150,1000)\n",
    "sig   = np.sqrt(float(Ns))\n",
    "gauss = 1./np.sqrt(2*np.pi)/sig*np.exp(-0.5*(xx/sig)**2)\n",
    "ax.plot(xx,gauss)\n",
    "#\n",
    "Nw = 1000\n",
    "Ns = 2000\n",
    "fp = final_position(Nw,Ns)\n",
    "ax.hist(fp,bins=nbin,density=True,alpha=0.5)\n",
    "#\n",
    "xx    = np.linspace(-150,150,1000)\n",
    "sig   = np.sqrt(float(Ns))\n",
    "gauss = 1./np.sqrt(2*np.pi)/sig*np.exp(-0.5*(xx/sig)**2)\n",
    "ax.plot(xx,gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a constraint ###\n",
    "\n",
    "Ok, so now let's put in a constraint -- in our case this will be a \"canal\" at $x=30$.  If the drunk reaches the canal they are removed from the problem.  With our Monte-Carlo approach this is almost completely trivial ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a canal at x_canal.\n",
    "#\n",
    "x_canal = 30.\n",
    "#\n",
    "# Need to modify the routine for \"final position\" so that if the\n",
    "# drunk's position crosses x_canal at any point that \"walker\" is\n",
    "# removed from the problem.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compare the distribution above to the analytic expectation derived\n",
    "# as in class for the Press-Schechter problem.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy tails ##\n",
    "\n",
    "Now let us show that the distribution of returns times (the number of steps required to return to the same level) has a power-law tail, specifically that the probability that the random walk returns to $0$ after $n$ steps has a power-law.  Power-law distributions are of great interest in complexity theory, network theory, self-organized criticality etc.  For example, we can use the above to argue that the random walk model implies the distribution of glacial lakes should be a power-law (as it is observed to be).  Ignoring the problem of \"lakes within lakes\".\n",
    "\n",
    "This random walk model is actually very interesting from a number of different aspects.  The insights we need all arise from Pascal's triangle, or the binomial distribution.  For $n$ steps the number of walks is $2^n$. The number of ways of getting to zero is the number of ways of choosing equal numbers of \"up\" and \"down\" steps, given equal probability, i.e.~$n$ choose $n/2$ ($n$ must be even). The probability that the $n^{\\rm th}$ step arives at zero is thus\n",
    "\\begin{equation}\n",
    "  P(S_n=0) = 2^{-n} \\left(\\begin{array}{c} n \\\\ n/2 \\end{array}\\right)\n",
    "           = 2^{-n} \\frac{n!}{(n/2)!(n/2)!}\n",
    "  \\to \\left(\\frac{2}{n\\pi}\\right)\\quad {\\rm as}\\ n\\to\\infty\n",
    "\\end{equation}\n",
    "where we have made use of the independence of each step and used Stirling's formula for the asymptotic limit. (In probability theory the idea that past events can't help predict the future is known as a \"martingale\", in the use of Stirling's formula you need to include the $\\mathcal{O}(\\ln n)$ term.) Glacial lakes are formed by the motion of glaciers over a landscape. As it moves the glacier can dip down and dig more deeply into the surface, or it can rise up and dig less deeply.  A lake is formed in a depression bounded by two regions of the same height.  This size of the lake is clearly given by the return time distribution of a random walk -- as observed.  There is a well-known problem in the Press-Schechter theory known as the \"halo within halo\" problem (analogous to the \"lakes within lakes\" issue!).  When doing the accounting I need to not count a low mass halo within a region which will collapse to form a larger mass halo.  The random walk picture does this for us, but if I try to go directly from the statistics of the initial field this is more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Modify the code above to compute the P(0) as a function of Nstep and\n",
    "# show it has a power-law behavior.  Note if you choose your array of\n",
    "# options to be [-1,1] rather than [-1.0,1.0] as I did then you can\n",
    "# avoid any floating point roundoff error.\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
